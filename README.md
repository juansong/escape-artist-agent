# ðŸŽ® Escape Artist Agent

An implementation of Monte Carlo Control for a custom escape-tactics gridworld.
The agent learns to dodge randomly placed <code style="background-color: yellow;">traps</code> and navigate uncertain environments 
to reach the <code style="background-color: blue;">extraction point</code> through trial and error.
This project demonstrates how reinforcement learning (RL) - specifically on/off policy Monte Carlo methods - can be applied to game AI design, 
with Q-learning baseline for comparison.

<p align="center">
    <img src="assets/rollout_success.gif" alt="Successful greedy rollout on a fixed layout" width="360">
    <img src="assets/rollout_failed.gif"  alt="Failed greedy rollout on a fixed layout"     width="360">
</p>
<p align="center"><em>Greedy policy rollouts across random trap layouts (successful/failed runs).</em></p>

---

## ðŸš€ Features
- âœ… **Algorithms**
    - On-policy **Monte Carlo Control**: First-Visit & Every-Visit
    - **Off-policy MC**: ordinary & weighted **importance sampling**
    - **Q-Learning** baseline
- âœ… **Environment**
    - Gridworld with **random trap generation** (exclusion zones + solvability check)
    - Configurable **trap density**, **slip**, **time limit**, **safe radius**
    - **per_env** (fixed) or **per_episode** (new layout each episode)

- âœ… Reproducible & visual
    - Saved artifacts: `Q.npy`, `returns.npy`, `params.json`, `config_used.yaml`
    - Plots: **learning curve**, **value heatmap** + **greedy arrows**, **trajectory overlay**
    - Helper scripts for **combined curves**, **hero GIF**, and **layout montage**

- âœ… Tests & tooling
    - `pytest` unit tests (env invariants + learning sanity)
    - **Optional CI** workflow
    - **Makefile** shortcuts

---

## ðŸ§© Environment
- **State**: Agent position on the grid `(x,y)`; optional obs modes (`"pos"`, `"pos_oneshot"`, `"full_grid"`)
- **Actions**: `0:up, 1:right, 2:down, 3:left`.
- **Dynamics**:
    - **Traps** sampled by density ('trap_pct') with exclusion zones near start & goal.
    - **Solvability** check (BFS); resample if no path exists.
    - Optional slip (action replaced by a random neighbor with prob `p_slip`).
- **Rewards**:
    - Step cost `-0.01`
    - Trap `-1.0` (terminal if lethal)
    - Goal `+1.0`
- **Termination**: Reaching goal, lethal trap, or time limit (`max_steps`).
- **API**: Gymnasium-style
    - `reset(seed) -> (obs, info)`
    - `step(a) -> (obs, r, terminated, truncated, info)`

---

## ðŸ“‚ Project Structure  
```
escape-artist-agent/
â”‚
â”œâ”€â”€ README.md                         <- Portfolio README
â”œâ”€â”€ pyproject.toml                    <- Package (installable library)
â”œâ”€â”€ requirements.txt                  <- (Optional) for quick install
â”œâ”€â”€ Makefile                          <- Shortcuts (train, figures, eval, clean)
â”œâ”€â”€ .gitignore                        <- Ignore runs/ artifacts, caches, editor files
â”œâ”€â”€ LICENSE                           <- GNU license
â”‚
â”œâ”€â”€ assets/                           <- Curated visuals & tables used in README
â”‚ â”œâ”€â”€ README.md
â”‚ â”œâ”€â”€ .gitkeep                        <- Placeholder file
â”‚ â””â”€â”€ figs/                           <- Outputs from notebook (value heatmap, rollout)
â”‚
â”œâ”€â”€ configs/                          <- Difficulty presets (env + algo defaults)
â”‚ â”œâ”€â”€ easy.yaml
â”‚ â”œâ”€â”€ medium.yaml
â”‚ â””â”€â”€ hard.yaml
â”‚
â”œâ”€â”€ escape_artist/                    <- Installable Python package (envs, algos, utils)
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ envs/                           <- Grid env + random trap generators
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ escape_env.py                 <- Gymnasium-style gridworld with traps/goal/slip
â”‚ â”‚ â””â”€â”€ generators.py                 <- Layout sampling, solvability(BFS), exclusion masks
â”‚ â”œâ”€â”€ algos/                          <- Learning algorithms
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ mc_control.py                 <- On-policy MC (first/every) + greedy rollout
â”‚ â”‚ â”œâ”€â”€ mc_offpolicy.py               <- Off-policy MC (importance sampling)
â”‚ â”‚ â””â”€â”€ q_learning.py                 <- Tabular Q-learning baseline
â”‚ â””â”€â”€ utils/
â”‚ â”œâ”€â”€ init.py
â”‚ â””â”€â”€ plotting.py                     <- Learning curve, value heatmap, rollout plots
â”‚
â”œâ”€â”€ experiments/                      <- CLI + analysis (run from repo root)
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ run_experiment.py               <- Train or re-plot from a saved run dir
â”‚ â”œâ”€â”€ evaluate.py                     <- Greedy eval â†’ CSV + Markdown (incl. timeout rate)
â”‚ â”œâ”€â”€ combine_curves.py               <- Merge multiple learning curves
â”‚ â”œâ”€â”€ make_hero_gif.py                <- Build demo GIF from rollout PNGs
â”‚ â”œâ”€â”€ make_layout_montage.py          <- Tile random layouts into a montage
â”‚ â”œâ”€â”€ ablations.py                    <- Abalation study - parameter sweeps (traps_pct Ã— slip)
â”‚ â””â”€â”€ analysis.ipynb                  <- Optional notebook for visuals/figures
â”‚
â”œâ”€â”€ tests/                            <- Pytest: env invariants + learning sanity
â”‚ â”œâ”€â”€ test_generators.py
â”‚ â”œâ”€â”€ test_env.py
â”‚ â”œâ”€â”€ test_mc.py
â”‚ â””â”€â”€ test_q_learning.py
â”‚
â”œâ”€â”€ docs/                             <- CLI walkthrough & Developer guide
â””â”€â”€ runs/                             <- Training outputs (gitignored; artifacts + figs/)

```
---


## âš™ï¸ Install

Python â‰¥ 3.9 recommended.

```bash
# Clone repository
git clone https://github.com/juansong/escape-artist-agent.git
cd escape-artist-agent

# Install (packages and dev tools)
python -m pip install --upgrade pip
python -m pip install -e .[dev]

```

## ðŸ› ï¸ Quickstart (CLI)

Train on **medium** and auto-save artifacts & plots:

```bash
# MC (Every-Visit)
python -m experiments.run_experiment --algo mc --visit every \
  --episodes 8000 --config configs/medium.yaml --out runs/mc_every_medium

# Off-policy MC (Weighted IS)
python -m experiments.run_experiment --algo mc_off --is weighted \
  --episodes 12000 --config configs/medium.yaml --out runs/mc_off_weighted_medium

# Q-Learning
python -m experiments.run_experiment --algo q \
  --episodes 8000 --config configs/medium.yaml --out runs/q_medium
```

Rebuild plots later (without retraining):

```bash
python -m experiments.run_experiment --plot --from runs/q_medium
```

Makefile shortcuts:

```bash
make install
make train-medium
make figures
```
---

## ðŸ“Š Results

#### ***Evaluation setup**
Unless noted otherwise we use `configs/medium.yaml`, `layout_mode=per_episode`, and evaluate greedy rollouts from learned Q-tables over 200 randomized layouts. â€œDetectionâ€ means the agent stepped on any trap at least once during an episode. Raw numbers are saved to <code>assets/eval_medium.csv</code> for reproducibility.

#### ***Note** 
Results emphasize **generalization**. We evaluate on **per-episode randomized layouts** (no map memorization). Medium uses 10Ã—10 grids, 10% traps, 10% slip, lethal traps. Failures are mostly trap hits; timeouts are reported explicitly.

### Evaluation metrics â€” Easy

*(6Ã—6, per-episode, lethal traps; 200 randomized layouts; greedy rollouts from learned Q)*

<!-- Paste the contents of assets/eval_easy.md below this line -->
| Method | Success Rate â†‘ | Avg Steps â†“ | Detection Rate â†“ | Timeout Rate â†“ |
| --- | --- | --- | --- | --- |
| MC (First) | 91.5% | 10.0 | 8.5% | 0.0% |
| Q-Learning | 88.0% | 10.0 | 12.0% | 0.0% |
<!-- END:EVAL_TABLE_EASY -->

[Raw CSV](assets/eval_easy.csv)

---

### Evaluation metrics â€” Medium

*(10Ã—10, per-episode, 10% traps, 10% slip, lethal; 200 randomized layouts; greedy rollouts from learned Q)*

| Method | Success Rate â†‘ | Avg Steps â†“ | Detection Rate â†“ |
| --- | --- | --- | --- |
| Every-Visit MC (12k) | 21.5% | 19.4 | 78.5% |
| MC-OFF (Weighted, 18k) | 20.5% | 19.8 | 79.5% |
| Q-Learning (12k) | 23.0% | 20.6 | 77.0% |


[Raw CSV](assets/eval_medium.csv)

---

### Learning curves (medium)

<img src="assets/curve_medium_mc_mc-off_q.png" alt="Learning curves: MC (Every), MC-OFF (Weighted), Q-Learning on medium" width="720">

---

### Value map & greedy policy (fixed layout)

<img src="assets/figs/value_heatmap_policy.png" alt="Value heatmap and greedy arrows on a fixed layout" width="420">

*(Generated on a fixed layout for readability; see `analysis.ipynb`.)*

---

### Layout diversity (per-episode sampling)

<img src="assets/layout_montage.png" alt="Montage of random trap layouts with goal marker" width="720">

---

### Ablations (optional)

Success rate versus **trap density** Ã— **slip**.

<table>
<tr>
<td><img src="assets/ablations_heatmap_mc.png" alt="MC Every-Visit ablation heatmap" width="360"></td>
<td><img src="assets/ablations_heatmap_q.png" alt="Q-Learning ablation heatmap" width="360"></td>
</tr>
</table>

---

### Reproduce key figures

```bash
# Train three baselines on medium
python -m experiments.run_experiment --algo mc --visit every --episodes 8000 --config configs/medium.yaml --out runs/mc_every_medium
python -m experiments.run_experiment --algo mc_off --is weighted --episodes 12000 --config configs/medium.yaml --out runs/mc_off_weighted_medium
python -m experiments.run_experiment --algo q --episodes 8000 --config configs/medium.yaml --out runs/q_medium

# Evaluation table (+ CSV and markdown)
python -m experiments.evaluate \
  --config configs/medium.yaml --episodes 200 \
  --runs runs/mc_every_medium runs/mc_off_weighted_medium runs/q_medium \
  --labels "Every-Visit MC" "MC-OFF (Weighted)" "Q-Learning" \
  --out_csv assets/eval_medium.csv --out_md assets/eval_medium.md

# Combined learning curves
python -m experiments.combine_curves \
  --runs runs/mc_every_medium runs/mc_off_weighted_medium runs/q_medium \
  --labels "MC (Every)" "MC-OFF (Weighted)" "Q-Learning" \
  --out assets/curve_medium_mc_mc-off_q.png

# Fixed-layout overlays (produced via analysis notebook)
# -> assets/figs/value_heatmap_policy.png and assets/figs/greedy_rollout.png

# Layout montage
python -m experiments.make_layout_montage \
  --rows 3 --cols 4 --size 10 10 --traps_pct 0.10 --slip 0.1 \
  --out assets/layout_montage.png

```

---
## ðŸ“’ Notebooks

`experiments/analysis.ipynb` enables:
- combine learning curves
- render value heatmaps + rollouts on a fixed layout
- preview ablation CSVs

---

## ðŸ§ª Tests
```bash
pytest -q
```

Covers:
- layout generation invariants
- env setup/termination/slip behavior
- MC & Q-Learning sanity on easy settings

---

## ðŸ“‚ Documentation
- **[Full CLI walkthrough](docs/CLI_WALKTHROUGH.md)** â€” end-to-end training, plotting, evaluation (easy + medium), GIFs, and tips.
- **[Developer Guide](docs/DEVELOPER_GUIDE.md)** - full docstrings and additional notes.
- **[PPT](docs/underprogress.pptx)** - portfolio ppt.

--------