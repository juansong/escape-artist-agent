# ğŸ® Monte Carlo Stealth Agent  

An implementation of **Monte Calro control** for a custom escape tactics game environment.
The agent learns to escape a grid world, avoid guards, and reach the extraction point through trial and error.

This project demonstrates how **reinforcement learning (RL)** - specifically **on-policy and off-policy Monte Carlo methods** - can be applied to
**game AI design**

---

## ğŸš€ Features  
- âœ… On-policy **First-Visit** and **Every-Visit Monte Carlo Control**  
- âœ… Off-policy Monte Carlo with **Importance Sampling**  
- âœ… Custom **grid-based escape environment** with guards, traps, and goals  
- âœ… Visualizations: Q(s,a) heatmaps, trajectory overlays, episodic return plots  
- âœ… Comparisons with **Q-learning** for benchmarking  

---

## ğŸ› ï¸ Installation  

Clone the repo and install dependencies:  

```bash
git clone https://github.com/your-username/mc-stealth-agent.git
cd mc-stealth-agent
pip install -r requirements.txt
```

## â–¶ï¸ Usage

### Training the Agent  

Run Monte Carlo training:  

```bash
python experiments/train_mc.py --episodes 5000
```

Adjust hyperparameters in `experiments/config.yaml`.

### Evaluating Policies

```bash
python experiments/evaluate.py --model saved_models/mc_policy.pkl
```

This will run the agent with the trained policy and log metrics such as:

- Success rate
- Average steps per episode
- Detection rate
---

## ğŸ“Š Analysis & Visualization  

The `notebooks/` folder contains tools to:  
- Plot **episodic return distributions**  
- Generate **Q(s,a) heatmaps**  
- Overlay **learned trajectories** vs scripted baselines  

Example heatmap visualization:  

<!-- <p align="center">  
  <img src="docs/q_heatmap.png" width="500"/>  
</p>   -->
---

## ğŸ¥ Demo  

<!-- <p align="center">  
  <img src="docs/demo.gif" width="500"/>  
</p>   -->

The demo shows the agentâ€™s progression:  
- **Episode 1:** Random policy, frequent guard detection  
- **Episode 500:** Learns safer detours and risk avoidance  
- **Episode 5000:** Consistently reaches the goal with minimal steps  
---

## ğŸ“ˆ Results  

| Method                  | Success Rate â†‘ | Avg Steps â†“ | Detection Rate â†“ |  
|--------------------------|----------------|-------------|------------------|  
| First-Visit MC           | 72%            | 18.4        | 12%              |  
| Every-Visit MC           | 76%            | 17.9        | 10%              |  
| Off-Policy MC (IS)       | 80%            | 16.7        | 9%               |  
| Q-Learning (baseline)    | 69%            | 19.5        | 15%              |  

**Key insights:**  
- Monte Carlo control learns safe navigation paths but requires many episodes.  
- Off-policy MC with importance sampling leverages scripted/human play data for faster convergence.  
- Reward shaping significantly influences the agentâ€™s stealth style (riskier but faster vs. safer but slower).  

---


## ğŸ“‚ Project Structure  

```
escape-artist-agent/
â”‚
â”œâ”€â”€ README.md                 <- Project overview, install, demo, etc.
â”œâ”€â”€ environment/
â”‚ â”œâ”€â”€ escape_env.py           <- Gym-style stealth environment
â”‚ â”œâ”€â”€ utils.py                <- Helpers: reward shaping, map loading
â”‚ â””â”€â”€ maps/                   <- ASCII/JSON maps
â”‚
â”œâ”€â”€ agent/
â”‚ â”œâ”€â”€ monte_carlo.py          <- First-Visit MC control implementation
â”‚ â”œâ”€â”€ policies.py             <- Îµ-soft policies, greedy updates
â”‚ â””â”€â”€ importance_sampling.py  <- Off-policy MC
â”‚
â”œâ”€â”€ experiments/
â”‚ â”œâ”€â”€ train_mc.py             <- Training script
â”‚ â”œâ”€â”€ evaluate.py             <- Evaluation script
â”‚ â”œâ”€â”€ ablations.py            <- Comparisons (MC vs Q-learning)
â”‚ â””â”€â”€ config.yaml             <-  Hyperparameters
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ analysis.ipynb          <- Training curves, returns
â”‚ â””â”€â”€ q_heatmaps.ipynb        <- Q(s,a) heatmaps
â”‚
â”œâ”€â”€ logs/                     <- Training logs, CSVs
â””â”€â”€ docs/                     <- Images, GIFs, figures for README
```

--------