# ğŸ® Monte Carlo Stealth Agent  

An implementation of **Monte Calro control** for a custom escape tactics game environment.
The agent learns to escape a grid world, avoid guards, and reach the extraction point through trial and error.

This project demonstrates how **reinforcement learning (RL)** - specifically **on-policy and off-policy Monte Carlo methods** - can be applied to
**game AI design**

---

## ğŸš€ Features  
- âœ… On-policy **First-Visit** and **Every-Visit Monte Carlo Control**  
- âœ… Off-policy Monte Carlo with **Importance Sampling**  
- âœ… Custom **grid-based escape environment** with guards, traps, and goals  
- âœ… Visualizations: Q(s,a) heatmaps, trajectory overlays, episodic return plots  
- âœ… Comparisons with **Q-learning** for benchmarking  

---

## ğŸ› ï¸ Installation  

Clone the repo and install dependencies:  

```bash
git clone https://github.com/your-username/mc-stealth-agent.git
cd mc-stealth-agent
pip install -r requirements.txt
```

## â–¶ï¸ Usage

### Training the Agent  

Run Monte Carlo training:  

```bash
python experiments/train_mc.py --episodes 5000
```

Adjust hyperparameters in `experiments/config.yaml`.

### Evaluating Policies

```bash
python experiments/evaluate.py --model saved_models/mc_policy.pkl
```

This will run the agent with the trained policy and log metrics such as:

- Success rate
- Average steps per episode
- Detection rate
---

## ğŸ“Š Analysis & Visualization  

The `notebooks/` folder contains tools to:  
- Plot **episodic return distributions**  
- Generate **Q(s,a) heatmaps**  
- Overlay **learned trajectories** vs scripted baselines  

Example heatmap visualization:  

<!-- <p align="center">  
  <img src="docs/q_heatmap.png" width="500"/>  
</p>   -->
---

## ğŸ¥ Demo  

<!-- <p align="center">  
  <img src="docs/demo.gif" width="500"/>  
</p>   -->

The demo shows the agentâ€™s progression:  
- **Episode 1:** Random policy, frequent guard detection  
- **Episode 500:** Learns safer detours and risk avoidance  
- **Episode 5000:** Consistently reaches the goal with minimal steps  
---

## ğŸ“ˆ Results  

| Method                  | Success Rate â†‘ | Avg Steps â†“ | Detection Rate â†“ |  
|--------------------------|----------------|-------------|------------------|  
| First-Visit MC           | 72%            | 18.4        | 12%              |  
| Every-Visit MC           | 76%            | 17.9        | 10%              |  
| Off-Policy MC (IS)       | 80%            | 16.7        | 9%               |  
| Q-Learning (baseline)    | 69%            | 19.5        | 15%              |  

**Key insights:**  
- Monte Carlo control learns safe navigation paths but requires many episodes.  
- Off-policy MC with importance sampling leverages scripted/human play data for faster convergence.  
- Reward shaping significantly influences the agentâ€™s stealth style (riskier but faster vs. safer but slower).  

---


## ğŸ“‚ Project Structure  

```
â”œâ”€â”€ LICENSE            <- Open-source license if one is chosen
â”œâ”€â”€ README.md          <- The top-level README for developers using this project
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ external       <- Data from third party sources
â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed
â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling
â”‚   â””â”€â”€ raw            <- The original, immutable data dump
â”‚
â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
â”‚                         the creator's initials, and a short `-` delimited description, e.g.
â”‚                         `1.0-jqp-initial-data-exploration`
â”‚
â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials
â”‚
â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
â”‚
â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
â”‚                         generated with `pip freeze > requirements.txt`
â”‚
â””â”€â”€ src                         <- Source code for this project
    â”‚
    â”œâ”€â”€ __init__.py             <- Makes src a Python module
    â”‚
    â”œâ”€â”€ config.py               <- Store useful variables and configuration
    â”‚
    â”œâ”€â”€ dataset.py              <- Scripts to download or generate data
    â”‚
    â”œâ”€â”€ features.py             <- Code to create features for modeling
    â”‚
    â”‚    
    â”œâ”€â”€ modeling                
    â”‚   â”œâ”€â”€ __init__.py 
    â”‚   â”œâ”€â”€ predict.py          <- Code to run model inference with trained models          
    â”‚   â””â”€â”€ train.py            <- Code to train models
    â”‚
    â”œâ”€â”€ plots.py                <- Code to create visualizations 
    â”‚
    â””â”€â”€ services                <- Service classes to connect with external platforms, tools, or APIs
        â””â”€â”€ __init__.py 
```

--------
