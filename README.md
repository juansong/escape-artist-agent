# ğŸ® Escape Artist Agent  

An implementation of **Monte Carlo control** for a custom escape tactics game environment.  
The agent learns to escape a grid world, avoid traps, and reach the goal through trial and error.

This project demonstrates how **reinforcement learning (RL)** â€” specifically **on-policy and off-policy Monte Carlo methods** â€” can be applied to **game AI design**.

---


## ğŸš€ Features  
- âœ… On-policy **First-Visit** and **Every-Visit Monte Carlo Control**  
- âœ… Off-policy Monte Carlo with **Importance Sampling**  
- âœ… Custom **grid-based escape environment** with guards, traps, and goals  
- âœ… Visualizations: Q(s,a) heatmaps, trajectory overlays, episodic return plots  
- âœ… Comparisons with **Q-learning** for benchmarking  

---

## ğŸ› ï¸ Quick Usage

### 1ï¸âƒ£ Install dependencies

Clone the repo and install dependencies:  

```bash
git clone https://github.com/juansong/mc-stealth-agent.git
cd escape-artist-agent
pip install -r requirements.txt
```

### 2ï¸âƒ£ Train the agent

```bash
python experiments/train_mc.py
```
Training logs and Q-table are saved in `logs/`

### 3ï¸âƒ£ Evaluate the learned policy

```bash
python experiments/evaluate.py
```
Reports success rate, average steps, and trap encounter rate

### 4ï¸âƒ£ Run a demo

```bash
python demo.py
```
saves `docs/escape_demo.gif` showing the agent navigating the grid

### 5ï¸âƒ£ Analyze results

```bash
jupyter notebook notebooks/analysis.ipynb
jupyter notebook notebooks/q_heatmaps.ipynb
```
---

## ğŸ“‚ Project Structure  
```
escape-artist-agent/
â”‚
â”œâ”€â”€ README.md                     <- Full portfolio README (intro, usage, demo, results)
â”œâ”€â”€ requirements.txt              <- All dependencies with tested versions
â”‚
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ escape_env.py             <- Custom Escape environment with random traps
â”‚   â”œâ”€â”€ utils.py                  <- Helpers: reward shaping, map loading
â”‚   â””â”€â”€ maps/                     <- ASCII/JSON maps
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ monte_carlo.py            <- First-Visit Monte Carlo agent
â”‚   â”œâ”€â”€ policies.py               <- Îµ-soft policies, greedy updates
â”‚   â””â”€â”€ importance_sampling.py    <- Off-policy Monte Carlo methods
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ train_mc.py               <- Training script (saves Q-table + training log)
â”‚   â”œâ”€â”€ evaluate.py               <- Policy evaluation script
â”‚   â”œâ”€â”€ ablations.py              <- Comparisons: MC vs Q-learning
â”‚   â””â”€â”€ config.yaml               <- Hyperparameters
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ analysis.ipynb            <- Plot training curves, average returns
â”‚   â””â”€â”€ q_heatmaps.ipynb          <- Visualize Q(s,a) heatmaps
â”‚
â”œâ”€â”€ logs/                         <- Automatically saved during training
â”‚   â”œâ”€â”€ training_log.csv          <- Episode rewards per training run
â”‚   â””â”€â”€ q_table.pkl               <- Saved Q-values for analysis/heatmaps
â”‚
â”œâ”€â”€ docs/                         <- Demo and analysis visuals
â”‚   â”œâ”€â”€ training_rewards.png      <- Example reward curve
â”‚   â””â”€â”€ escape_demo.gif           <- Example GIF of trained agent
â”‚
â””â”€â”€ demo.py                        <- Runs trained agent and generates GIF
```
---

## ğŸ“Š Results

### Training Performance
**Reward progression over episodes**:

![Training Rewards](docs/training_rewards.png)

- Shows how the agent learns to maximize cumulative reward  
- Average return over the last 100 episodes indicates stable policy  

---

### Evaluation Metrics

| Method                  | Success Rate â†‘ | Avg Steps â†“ | Detection Rate â†“ |
|--------------------------|---------------|-------------|-----------------|
| First-Visit MC           | 72%           | 18.4        | 12%             |
| Every-Visit MC           | 76%           | 17.9        | 10%             |
| Off-Policy MC (IS)       | 80%           | 16.7        | 9%              |
| Q-Learning (baseline)    | 69%           | 19.5        | 15%             |

**Key insights:**  
- Monte Carlo control learns **safe navigation paths** but requires many episodes to converge.  
- **Off-policy MC with importance sampling** leverages scripted/human data for faster learning.  
- **Reward shaping** significantly influences the agentâ€™s style: riskier paths reach the goal faster but with higher detection risk, whereas safer paths take longer but minimize detection.  

---

### Demo Episode
A trained agent escaping traps and reaching the goal:

![Escape Demo](docs/escape_demo.gif)
---

## ğŸ““ Analysis
Notebooks for deeper exploration:
- **analysis.ipynb**   -> reward curves, moving averages, average return
- **q_heatmaps.ipynb** -> heatmaps of Q(s,a) values to visualize learned policy
---

## ğŸ› ï¸ Dependencies
```text
gym
numpy
matplotlib
seaborn
pandas
imageio
tqdm
```

Install via:
```bash
pip install -r requirements.txt
```
---

## ğŸ“– Notes
- **Python version**: 3.9+ recommended
- To install:
```bash
pip install -r requirements.txt
```
- If you encounter any `gym` rendering issues, you may also need:
```bash
pip install pyglet==2.3.2
```

- Traps are randomly generated each episode; the agent learns a robust policy
- The Q-table and reward logs allow full reproducibility and analysis
- GIF demo provides visual proof of the agentâ€™s learning

--------