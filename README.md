# ğŸ® Escape Artist Agent  

An implementation of **Monte Carlo control** for a custom escape tactics game environment.  
The agent learns to escape a grid world, avoid traps, and reach the extraction point through trial and error.

This project demonstrates how **reinforcement learning (RL)** â€” specifically **on-policy and off-policy Monte Carlo methods** â€” can be applied to **game AI design**.

---

## ğŸš€ Features  
- âœ… On-policy **First-Visit** and **Every-Visit Monte Carlo Control**  
- âœ… Off-policy Monte Carlo with **Importance Sampling**  
- âœ… Custom **grid-based escape environment** with guards, traps, and goals  
- âœ… Visualizations: Q(s,a) heatmaps, trajectory overlays, episodic return plots  
- âœ… Comparisons with **Q-learning** for benchmarking  

---

## âš™ï¸ Installation
```bash
# Clone repository
git clone https://github.com/juansong/escape-artist-agent.git
cd escape-artist-agent

# Create Conda environment
conda env create -f environment.yml
conda activate escape-agent

# Or install via pip (recommended)
pip install -r requirements.txt

```

## ğŸ› ï¸ Quick Usage

### 1ï¸âƒ£ Train agent
```bash
python -m experiments.train_mc
```
- Saves `logs/q_table.pkl` and `logs/training_log.csv`.

### 2ï¸âƒ£ Evaluate trained policy

```bash
python -m expeeriments.evaluate
```
- Computes **Success Rate**, **Average Steps**, **Detection Rate**.

### 3ï¸âƒ£ Run demo

```bash
python -m demo
```
- Displays trained agent navigating the grid.
- Generates GIF in `docs/escape_demo.gif`.
---

## ğŸ“Š Results

### Training Performance
**Reward progression over episodes (reward curve)**:

![Training Rewards](docs/training_rewards.png)

- Shows how the agent learns to maximize cumulative reward  
- Average return over the last 100 episodes indicates stable policy  

### Evaluation Metrics

| Method                  | Success Rate â†‘ | Avg Steps â†“ | Detection Rate â†“ |
|--------------------------|---------------|-------------|-----------------|
| First-Visit MC           | 72%           | 18.4        | 12%             |
| Every-Visit MC           | 76%           | 17.9        | 10%             |
| Off-Policy MC (IS)       | 80%           | 16.7        | 9%              |
| Q-Learning (baseline)    | 69%           | 19.5        | 15%             |

**Key insights:**  
- Monte Carlo control learns **safe navigation paths** but requires many episodes to converge.  
- **Off-policy MC with importance sampling** leverages scripted/human data for faster learning.  
- **Reward shaping** significantly influences the agentâ€™s style: riskier paths reach the goal faster but with higher detection risk, whereas safer paths take longer but minimize detection.  

---

## ğŸ§ªAnalysis & Visualization

```bash
jupyter notebook notebooks/analysis.ipynb
jupyter notebook notebooks/q_heatmaps.ipynb
```
- Use `notebooks/analysis.ipynb` to plot **episodic reward progression**.
- Use `notebooks/q_heatmaps.ipynb` to visualize **Q(s,a) heatmaps**.
- Metrics are logged in `logs/training_log.csv` for reproducibilty.

---

## ğŸ“‚ Project Structure  
```
escape-artist-agent/
â”‚
â”œâ”€â”€ README.md                         <- Full portfolio README (intro, usage, demo, results)
â”œâ”€â”€ requirements.txt                  <- All dependencies with tested versions
â”‚
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ escape_env.py                 <- Custom Escape environment with random traps
â”‚   â”œâ”€â”€ utils.py                      <- Helpers: reward shaping, map loading
â”‚   â””â”€â”€ maps/                         <- ASCII/JSON maps
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ monte_carlo.py                <- First-Visit Monte Carlo agent
â”‚   â”œâ”€â”€ policies.py                   <- Îµ-soft policies, greedy updates
â”‚   â””â”€â”€ importance_sampling.py        <- Off-policy Monte Carlo methods
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ train_mc.py                   <- Training script (saves Q-table + training log)
â”‚   â”œâ”€â”€ evaluate.py                   <- Policy evaluation script
â”‚   â”œâ”€â”€ ablations.py                  <- Comparisons: MC vs Q-learning
â”‚   â””â”€â”€ config.yaml                   <- Hyperparameters
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ analysis.ipynb                <- Plot training curves, average returns
â”‚   â””â”€â”€ q_heatmaps.ipynb              <- Visualize Q(s,a) heatmaps
â”‚
â”œâ”€â”€ logs/                             <- Automatically saved during training
â”‚   â”œâ”€â”€ training_log.csv              <- Episode rewards per training run
â”‚   â””â”€â”€ q_table.pkl                   <- Saved Q-values for analysis/heatmaps
â”‚
â”œâ”€â”€ docs/                         
â”‚   â”œâ”€â”€ training_rewards.png          <- Demo reward curve
â”‚   â”œâ”€â”€ escape_demo.gif               <- Demo GIF of trained agent
â”‚   â””â”€â”€ dependency_graph.png          <- Dependency graph
â”‚
â”œâ”€â”€ scripts/                         
â”‚   â””â”€â”€ generate_dependency_graph.py  <- Generate ependency graph of each modules
â”‚
â””â”€â”€ demo.py                           <- Runs trained agent and generates GIF
```
---

## ğŸ“‚ Code Dependency Graph

```bash
# Install Graphviz (macOS, Windows)
brew install graphviz
choco install graphviz

# Verify after installation
dot -V

# Run the script
python scripts/generate_dependency_graph.py
```

The graph illustrates how different modules in this project interact.

- **Main scripts** (`train_mc.py`, `evaluate.py`, `demo.py`) handle execution.
- **Agent modules** (`monte_carlo.py`, `importance_sampling.py`, `policies.py`) contain the logic.
- **Environment modules** (`escape_env.py`, `utils.py`) simulate the grid world, manage maps, and handle reward shaping.
- **Benchmark module** (`Ablations.py`) enables benchmarking and comparisons between methods.
- **Arrows** indicate import or usage dependencies.

![Dependency Graph](docs/dependency_graph.png)

---

### Demo
Demo Episode: A trained agent escaping the environment avoiding traps and reaching the goal:

```bash
python -m demo
```

![Escape Demo](docs/escape_demo.gif)
---


## ğŸ“– Notes
- **Python version**: 3.9+ recommended
- Traps are randomly generated each episode; the agent learns a robust policy
- The Q-table and reward logs allow full reproducibility and analysis
- GIF demo provides visual proof of the agentâ€™s learning

--------